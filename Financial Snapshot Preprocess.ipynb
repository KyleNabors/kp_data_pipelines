{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bab7a7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89660aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J:\\ADMIN-eFILES\\CHEN_W154867_VXC\\z_Reports\\Monthly Operating Statements\\2025\\Cumulative Report - Operating Statements - 1025 - Hard Coded.xlsx\n",
      "2025-10-31\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "base = Path(r\"J:\\ADMIN-eFILES\\CHEN_W154867_VXC\\z_Reports\\Monthly Operating Statements\")\n",
    "\n",
    "year_dirs = [p for p in base.iterdir() if p.is_dir() and p.name.isdigit()]\n",
    "if not year_dirs:\n",
    "    raise FileNotFoundError(f\"No year folders found under {base!s}\")\n",
    "latest_year_dir = max(year_dirs, key=lambda p: int(p.name))\n",
    "\n",
    "pattern_files = list(\n",
    "    latest_year_dir.glob(\"Cumulative Report - Operating Statements - *.xlsx\")\n",
    ")\n",
    "xlsx_files = pattern_files or list(latest_year_dir.glob(\"*.xlsx\"))\n",
    "if not xlsx_files:\n",
    "    raise FileNotFoundError(f\"No .xlsx files found in {latest_year_dir!s}\")\n",
    "\n",
    "latest_report = max(xlsx_files, key=lambda p: p.stat().st_mtime)\n",
    "\n",
    "report_path = latest_report\n",
    "print(report_path)\n",
    "\n",
    "dor = pd.read_excel(report_path, sheet_name=\"Cumulative Report\", skiprows=8)\n",
    "\n",
    "\n",
    "dor_end_date = report_path.stem.split(\" - \")[-2]\n",
    "\n",
    "dor_end_date = (\n",
    "    pd.to_datetime(dor_end_date, format=\"%m%y\", errors=\"raise\")\n",
    "    .to_period(\"M\")\n",
    "    .to_timestamp(\"M\")\n",
    "    .date()\n",
    ")\n",
    "\n",
    "print(dor_end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fb623b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project ID                                            object\n",
      "Project Title                                         object\n",
      "Program Area                                          object\n",
      "Funder Type                                           object\n",
      "Principal Investigator (PI)                           object\n",
      "Award Term Start Date                         datetime64[ns]\n",
      "Project Status                                        object\n",
      "Total Cash Receipts                                  float64\n",
      "Total Personnel                                      float64\n",
      "Total Contractual/\\nOutside Services Costs           float64\n",
      "Total \\nNon-Personnel                                float64\n",
      "Total Cost                                           float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# drop Unnamed: 0 only if it exists\n",
    "if \"Unnamed: 0\" in dor.columns:\n",
    "    dor = dor.drop(columns=[\"Unnamed: 0\"])\n",
    "\n",
    "# desired columns (use canonical names)\n",
    "desired_cols = [\n",
    "    \"Project ID\",\n",
    "    \"Project Title\",\n",
    "    \"Program Area\",\n",
    "    \"Funder Type\",\n",
    "    \"Principal Investigator (PI)\",\n",
    "    \"Award Term Start Date\",\n",
    "    \"Project Status\",\n",
    "    \"Total Cash Receipts\",\n",
    "    \"Total Personnel\",\n",
    "    \"Total Contractual/ Outside Services Costs\",\n",
    "    \"Total Non-Personnel\",\n",
    "    \"Total Cost\",\n",
    "]\n",
    "\n",
    "# normalize helper to match columns ignoring whitespace/newlines/case\n",
    "normalize = lambda s: \"\".join(s.split()).lower() if isinstance(s, str) else s\n",
    "col_map = {normalize(c): c for c in dor.columns}\n",
    "\n",
    "# build selected column list from available columns (skip missing ones)\n",
    "selected = []\n",
    "missing = []\n",
    "for c in desired_cols:\n",
    "    key = normalize(c)\n",
    "    if key in col_map:\n",
    "        selected.append(col_map[key])\n",
    "    else:\n",
    "        missing.append(c)\n",
    "\n",
    "if missing:\n",
    "    print(\n",
    "        f\"Warning: these desired columns were not found and will be skipped: {missing}\"\n",
    "    )\n",
    "\n",
    "# subset dataframe to the selected (available) columns\n",
    "dor = dor[selected]\n",
    "\n",
    "print(dor.dtypes)\n",
    "\n",
    "dor.to_excel(\n",
    "    \"C:\\\\Users\\\\O304312\\\\OneDrive - Kaiser Permanente\\\\Documents\\\\Tableau Dashboards\\\\New Financial Snapshot\\\\Data\\\\DOR Data Preprocessed.xlsx\",\n",
    "    index=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7afa37c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: J:\\ADMIN-eFILES\\CHEN_W154867_VXC\\z_Reports\\Transaction Detail\\CTP Transaction Detail 103125.xlsx\n",
      "Columns: ['Project ID', 'NUID', 'Name', '2014', '2015', '2016', '2017', '2018', '2019', '2020', '2021', '2022', '2023', '2024', '2025_Qtr1_Jan', '2025_Qtr1_Feb', '2025_Qtr1_Mar', '2025_Qtr2_Apr', '2025_Qtr2_May', '2025_Qtr2_Jun', '2025_Qtr3_Jul', '2025_Qtr3_Aug', '2025_Qtr3_Sep', '2025_Qtr4_Oct', '2025 Total', 'Grand Total']\n",
      "Loaded dataframe shape: (214, 26)\n"
     ]
    }
   ],
   "source": [
    "txn_base = Path(r\"J:\\ADMIN-eFILES\\CHEN_W154867_VXC\\z_Reports\\Transaction Detail\")\n",
    "pattern = \"CTP Transaction Detail *.xlsx\"\n",
    "matches = list(txn_base.glob(pattern))\n",
    "\n",
    "if matches:\n",
    "    ctp_path = max(matches, key=lambda p: p.stat().st_mtime)\n",
    "else:\n",
    "\n",
    "    fallback = txn_base / \"CTP Transaction Detail 103125.xlsx\"\n",
    "    if fallback.exists():\n",
    "        ctp_path = fallback\n",
    "    else:\n",
    "        raise FileNotFoundError(\n",
    "            f\"No files matching {pattern!s} and fallback {fallback!s} not found in {txn_base!s}\"\n",
    "        )\n",
    "\n",
    "print(\"Loading:\", ctp_path)\n",
    "\n",
    "ctp_hours = pd.read_excel(ctp_path, sheet_name=\"Hours\", header=[8, 9, 10, 11])\n",
    "\n",
    "\n",
    "def tidy(col):\n",
    "    parts = [\n",
    "        str(x).strip()\n",
    "        for x in col\n",
    "        if str(x).strip() not in {\"nan\", \"\"} and not str(x).startswith(\"Unnamed\")\n",
    "    ]\n",
    "    return \"_\".join(parts).strip(\"_\")\n",
    "\n",
    "\n",
    "ctp_hours.columns = [tidy(col) for col in ctp_hours.columns]\n",
    "\n",
    "ctp_hours = ctp_hours.rename(columns={\"Project\": \"Project ID\"})\n",
    "\n",
    "mask = ctp_hours[\"Project ID\"].astype(str).str.strip().str.lower().str.endswith(\"total\")\n",
    "ctp_hours = ctp_hours[mask].copy()\n",
    "ctp_hours.reset_index(drop=True, inplace=True)\n",
    "\n",
    "rng_mask = (\n",
    "    ctp_hours[\"Project ID\"].astype(str).str.match(r\"^(RNG\\d+)\\s+Total$\", na=False)\n",
    ")\n",
    "ctp_hours.loc[rng_mask, \"Project ID\"] = (\n",
    "    ctp_hours.loc[rng_mask, \"Project ID\"]\n",
    "    .astype(str)\n",
    "    .str.replace(r\"^(RNG\\d+)\\s+Total$\", r\"\\1\", regex=True)\n",
    "    .str.strip()\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Columns:\", ctp_hours.columns.tolist())\n",
    "\n",
    "print(\"Loaded dataframe shape:\", ctp_hours.shape)\n",
    "\n",
    "ctp_hours.to_excel(\n",
    "    \"C:\\\\Users\\\\O304312\\\\OneDrive - Kaiser Permanente\\\\Documents\\\\Tableau Dashboards\\\\New Financial Snapshot\\\\Data\\\\DOR Personnel.xlsx\",\n",
    "    index=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb41d445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: J:\\VIEWPOINT\\SiteStudyDetails_Response\\2025-12-04-SiteStudyDetails.json\n",
      "Converted to DataFrame with shape: (1078, 40)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "vp_folder = Path(\"J:\\\\VIEWPOINT\\\\SiteStudyDetails_Response\")\n",
    "\n",
    "pattern = re.compile(r\"^\\d{4}-\\d{2}-\\d{2}-SiteStudyDetails\\.json$\")\n",
    "\n",
    "matches = [p for p in vp_folder.iterdir() if p.is_file() and pattern.match(p.name)]\n",
    "if not matches:\n",
    "    raise FileNotFoundError(f\"No SiteStudyDetails json files found in {vp_folder!s}\")\n",
    "\n",
    "latest_json = max(matches, key=lambda p: p.stat().st_mtime)\n",
    "print(\"Loading:\", latest_json)\n",
    "\n",
    "with latest_json.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    site_details = json.load(f)\n",
    "\n",
    "try:\n",
    "    ss_df = pd.json_normalize(site_details)\n",
    "    print(\"Converted to DataFrame with shape:\", ss_df.shape)\n",
    "except Exception:\n",
    "    ss_df = None\n",
    "    print(\"JSON loaded into 'site_details' (not converted to DataFrame).\")\n",
    "\n",
    "vp_study_details = ss_df.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f49f7e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: J:\\VIEWPOINT\\Accountables_Response\\2025-12-04-Accountables.json\n",
      "Converted to DataFrame with shape: (12082, 38)\n"
     ]
    }
   ],
   "source": [
    "vp_accountables_folder = Path(\"J:\\\\VIEWPOINT\\\\Accountables_Response\")\n",
    "\n",
    "pattern = re.compile(r\"^\\d{4}-\\d{2}-\\d{2}-Accountables\\.json$\")\n",
    "\n",
    "matches = [\n",
    "    p for p in vp_accountables_folder.iterdir() if p.is_file() and pattern.match(p.name)\n",
    "]\n",
    "if not matches:\n",
    "    raise FileNotFoundError(\n",
    "        f\"No SiteStudyDetails json files found in {vp_accountables_folder!s}\"\n",
    "    )\n",
    "\n",
    "latest_json = max(matches, key=lambda p: p.stat().st_mtime)\n",
    "print(\"Loading:\", latest_json)\n",
    "\n",
    "with latest_json.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    site_details = json.load(f)\n",
    "\n",
    "try:\n",
    "    account = pd.json_normalize(site_details)\n",
    "    print(\"Converted to DataFrame with shape:\", account.shape)\n",
    "except Exception:\n",
    "    account = None\n",
    "    print(\"JSON loaded into 'site_details' (not converted to DataFrame).\")\n",
    "\n",
    "vp_accountables = account.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0ab9e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "vp_accountables = vp_accountables.merge(\n",
    "    vp_study_details[[\"site_study_service_line\", \"network_study_uuid\"]],\n",
    "    on=\"network_study_uuid\",\n",
    "    how=\"left\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c8e007a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse completion_date safely and compare against a Timestamp cutoff\n",
    "vp_accountables[\"completion_date_parsed\"] = pd.to_datetime(\n",
    "    vp_accountables[\"completion_date\"], errors=\"coerce\"\n",
    ")\n",
    "\n",
    "# ensure dor_end_date is a pandas Timestamp (not a python date)\n",
    "cutoff = pd.to_datetime(dor_end_date)\n",
    "\n",
    "vp_accountables = vp_accountables[\n",
    "    vp_accountables[\"completion_date_parsed\"] < cutoff\n",
    "].copy()\n",
    "\n",
    "# remove helper column\n",
    "vp_accountables.drop(columns=[\"completion_date_parsed\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2fdfa4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "vp_accountables = vp_accountables[[\"site_study_service_line\", \"amount\"]]\n",
    "vp_accountables = vp_accountables.groupby(\"site_study_service_line\").sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70dc039c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove control characters that cause openpyxl IllegalCharacterError\n",
    "# keep common whitespace (\\t, \\n, \\r); remove other < 0x20 controls\n",
    "ctl_re = re.compile(r\"[\\x00-\\x08\\x0b\\x0c\\x0e-\\x1f]\")\n",
    "for col in vp_accountables.select_dtypes(include=[\"object\"]).columns:\n",
    "    mask = vp_accountables[col].notna()\n",
    "    vp_accountables.loc[mask, col] = (\n",
    "        vp_accountables.loc[mask, col].astype(str).map(lambda s: ctl_re.sub(\"\", s))\n",
    "    )\n",
    "\n",
    "vp_accountables.to_excel(\n",
    "    r\"C:\\Users\\O304312\\OneDrive - Kaiser Permanente\\Documents\\Tableau Dashboards\\New Financial Snapshot\\Data\\Viewpoint Accountables.xlsx\",\n",
    "    index=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc93d6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "vp_study_details.to_excel(\n",
    "    r\"C:\\Users\\O304312\\OneDrive - Kaiser Permanente\\Documents\\Tableau Dashboards\\New Financial Snapshot\\Data\\Viewpoint Site Study Details.xlsx\",\n",
    "    index=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "852dc1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dor[\"ID\"] = dor[\"Project Title\"]\n",
    "\n",
    "program_col = next(\n",
    "    (c for c in dor.columns if normalize(c) == normalize(\"Program Area\")), None\n",
    ")\n",
    "\n",
    "if program_col is not None:\n",
    "\n",
    "    def _remove_prog(id_val, prog_val):\n",
    "        if pd.isna(id_val) or pd.isna(prog_val):\n",
    "            return id_val\n",
    "        id_s = str(id_val)\n",
    "        prog_s = str(prog_val).strip()\n",
    "        if not prog_s:\n",
    "            return id_s\n",
    "        out = re.sub(re.escape(prog_s), \"\", id_s, flags=re.IGNORECASE)\n",
    "        out = re.sub(r\"[\\-\\–\\—:;\\/]+\", \" \", out)\n",
    "        out = \" \".join(out.split()).strip()\n",
    "        return out\n",
    "\n",
    "    dor[\"ID\"] = dor.apply(lambda r: _remove_prog(r[\"ID\"], r[program_col]), axis=1)\n",
    "else:\n",
    "    print(\"Warning: 'Program Area' column not found in dor; ID left unchanged.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea7c219e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153 Project ID(s) not found in site_study_service_line (saved to file)\n"
     ]
    }
   ],
   "source": [
    "vp_set = set(\n",
    "    vp_study_details[\"site_study_service_line\"]\n",
    "    .dropna()\n",
    "    .astype(str)\n",
    "    .map(lambda s: \"\".join(s.split()).casefold())\n",
    ")\n",
    "\n",
    "dor_norm = dor[\"Project ID\"].astype(str).map(lambda s: \"\".join(s.split()).casefold())\n",
    "\n",
    "mask_missing = ~dor_norm.isin(vp_set)\n",
    "\n",
    "title_col = next(\n",
    "    (c for c in dor.columns if normalize(c) == normalize(\"Project Title\")), None\n",
    ")\n",
    "\n",
    "cols = [\"Project ID\"]\n",
    "if title_col:\n",
    "    cols.append(title_col)\n",
    "\n",
    "missing_df = dor.loc[mask_missing, cols].drop_duplicates().copy()\n",
    "\n",
    "vp_map = {}\n",
    "for sl, sid in (\n",
    "    vp_study_details[[\"site_study_service_line\", \"site_study_id\"]]\n",
    "    .dropna(subset=[\"site_study_service_line\"])\n",
    "    .itertuples(index=False)\n",
    "):\n",
    "    key = \"\".join(str(sl).split()).casefold()\n",
    "    if key and pd.notna(sid):\n",
    "        # keep first occurrence\n",
    "        vp_map.setdefault(key, sid)\n",
    "\n",
    "missing_df[\"site_study_id\"] = (\n",
    "    missing_df[\"Project ID\"]\n",
    "    .astype(str)\n",
    "    .map(lambda s: \"\".join(s.split()).casefold())\n",
    "    .map(vp_map)\n",
    ")\n",
    "\n",
    "if title_col:\n",
    "    missing_df[\"site_study_id\"] = missing_df[\"site_study_id\"].fillna(\n",
    "        missing_df[title_col]\n",
    "    )\n",
    "\n",
    "out_cols = [\"site_study_id\", \"Project ID\"]\n",
    "if title_col:\n",
    "    out_cols.append(title_col)\n",
    "\n",
    "missing_out = missing_df.loc[:, out_cols]\n",
    "\n",
    "print(\n",
    "    f\"{len(missing_out)} Project ID(s) not found in site_study_service_line (saved to file)\"\n",
    ")\n",
    "\n",
    "out_path = r\"C:\\Users\\O304312\\OneDrive - Kaiser Permanente\\Documents\\Tableau Dashboards\\New Financial Snapshot\\Data\\VP Missing RNG Numbers.xlsx\"\n",
    "missing_out.to_excel(out_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcfe1a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
