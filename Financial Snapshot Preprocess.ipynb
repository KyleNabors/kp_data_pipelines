{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bab7a7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "89660aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J:\\ADMIN-eFILES\\CHEN_W154867_VXC\\z_Reports\\Monthly Operating Statements\\2025\\Cumulative Report - Operating Statements - 1025 - Hard Coded.xlsx\n",
      "2025-10-31\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "base = Path(r\"J:\\ADMIN-eFILES\\CHEN_W154867_VXC\\z_Reports\\Monthly Operating Statements\")\n",
    "\n",
    "year_dirs = [p for p in base.iterdir() if p.is_dir() and p.name.isdigit()]\n",
    "if not year_dirs:\n",
    "    raise FileNotFoundError(f\"No year folders found under {base!s}\")\n",
    "latest_year_dir = max(year_dirs, key=lambda p: int(p.name))\n",
    "\n",
    "pattern_files = list(\n",
    "    latest_year_dir.glob(\"Cumulative Report - Operating Statements - *.xlsx\")\n",
    ")\n",
    "xlsx_files = pattern_files or list(latest_year_dir.glob(\"*.xlsx\"))\n",
    "if not xlsx_files:\n",
    "    raise FileNotFoundError(f\"No .xlsx files found in {latest_year_dir!s}\")\n",
    "\n",
    "latest_report = max(xlsx_files, key=lambda p: p.stat().st_mtime)\n",
    "\n",
    "report_path = latest_report\n",
    "print(report_path)\n",
    "\n",
    "dor = pd.read_excel(report_path, sheet_name=\"Summary - DC only\", skiprows=6)\n",
    "\n",
    "\n",
    "dor_end_date = report_path.stem.split(\" - \")[-2]\n",
    "\n",
    "dor_end_date = (\n",
    "    pd.to_datetime(dor_end_date, format=\"%m%y\", errors=\"raise\")\n",
    "    .to_period(\"M\")\n",
    "    .to_timestamp(\"M\")\n",
    "    .date()\n",
    ")\n",
    "\n",
    "print(dor_end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e8bff5e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project ID                                                               object\n",
      "OL Project Status                                                        object\n",
      "Start Date                                                       datetime64[ns]\n",
      "End Date                                                         datetime64[ns]\n",
      "Short \"Project\" Title                                                    object\n",
      "Principal Investigator                                                   object\n",
      "Funder Type                                                              object\n",
      "Invoice Type                                                             object\n",
      "Total PFSR Direct Budget                                                float64\n",
      "Current Month's\\nDirect Expenses                                          int64\n",
      "Total Direct Expenses                                                   float64\n",
      "Total PFSR Direct Budget \\nvs Total Direct\\nExpenses Variance           float64\n",
      "Total Direct Payments Received                                          float64\n",
      "Total Direct Payments vs. Total Direct Expenses Variance                float64\n",
      "Total Direct and Indirect Payments Received to Date                     float64\n",
      "Total Direct Income/(Loss) based on Invoice Type\\n                      float64\n",
      "Current Month's Payments Received\\n(Incl DAF)                           float64\n",
      "Outstanding Invoices                                                    float64\n",
      "Comments                                                                 object\n",
      "Unnamed: 19                                                             float64\n",
      "Project Area                                                             object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(dor.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5fb623b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop Unnamed: 0 only if it exists\n",
    "# if \"Unnamed: 0\" in dor.columns:\n",
    "#     dor = dor.drop(columns=[\"Unnamed: 0\"])\n",
    "\n",
    "# # desired columns (use canonical names)\n",
    "# desired_cols = [\n",
    "#     \"Project ID\",\n",
    "#     \"Project Title\",\n",
    "#     \"Program Area\",\n",
    "#     \"Funder Type\",\n",
    "#     \"Principal Investigator (PI)\",\n",
    "#     \"Award Term Start Date\",\n",
    "#     \"Project Status\",\n",
    "#     \"Total Cash Receipts\",\n",
    "#     \"Total Personnel\",\n",
    "#     \"Total Contractual/ Outside Services Costs\",\n",
    "#     \"Total Non-Personnel\",\n",
    "#     \"Total Cost\",\n",
    "# ]\n",
    "\n",
    "# # normalize helper to match columns ignoring whitespace/newlines/case\n",
    "# normalize = lambda s: \"\".join(s.split()).lower() if isinstance(s, str) else s\n",
    "# col_map = {normalize(c): c for c in dor.columns}\n",
    "\n",
    "# # build selected column list from available columns (skip missing ones)\n",
    "# selected = []\n",
    "# missing = []\n",
    "# for c in desired_cols:\n",
    "#     key = normalize(c)\n",
    "#     if key in col_map:\n",
    "#         selected.append(col_map[key])\n",
    "#     else:\n",
    "#         missing.append(c)\n",
    "\n",
    "# if missing:\n",
    "#     print(\n",
    "#         f\"Warning: these desired columns were not found and will be skipped: {missing}\"\n",
    "#     )\n",
    "\n",
    "# # subset dataframe to the selected (available) columns\n",
    "# dor = dor[selected]\n",
    "\n",
    "# print(dor.dtypes)\n",
    "\n",
    "dor.to_excel(\n",
    "    \"C:\\\\Users\\\\O304312\\\\OneDrive - Kaiser Permanente\\\\Documents\\\\Tableau Dashboards\\\\New Financial Snapshot\\\\Data\\\\DOR Data Preprocessed.xlsx\",\n",
    "    index=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df6f625",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7afa37c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: J:\\ADMIN-eFILES\\CHEN_W154867_VXC\\z_Reports\\Transaction Detail\\CTP Transaction Detail 103125.xlsx\n",
      "Columns: ['Project ID', 'NUID', 'Name', '2014', '2015', '2016', '2017', '2018', '2019', '2020', '2021', '2022', '2023', '2024', '2025_Qtr1_Jan', '2025_Qtr1_Feb', '2025_Qtr1_Mar', '2025_Qtr2_Apr', '2025_Qtr2_May', '2025_Qtr2_Jun', '2025_Qtr3_Jul', '2025_Qtr3_Aug', '2025_Qtr3_Sep', '2025_Qtr4_Oct', '2025 Total', 'Grand Total']\n",
      "Loaded dataframe shape: (214, 26)\n"
     ]
    }
   ],
   "source": [
    "txn_base = Path(r\"J:\\ADMIN-eFILES\\CHEN_W154867_VXC\\z_Reports\\Transaction Detail\")\n",
    "pattern = \"CTP Transaction Detail *.xlsx\"\n",
    "matches = list(txn_base.glob(pattern))\n",
    "\n",
    "if matches:\n",
    "    ctp_path = max(matches, key=lambda p: p.stat().st_mtime)\n",
    "else:\n",
    "\n",
    "    fallback = txn_base / \"CTP Transaction Detail 103125.xlsx\"\n",
    "    if fallback.exists():\n",
    "        ctp_path = fallback\n",
    "    else:\n",
    "        raise FileNotFoundError(\n",
    "            f\"No files matching {pattern!s} and fallback {fallback!s} not found in {txn_base!s}\"\n",
    "        )\n",
    "\n",
    "print(\"Loading:\", ctp_path)\n",
    "\n",
    "ctp_hours = pd.read_excel(ctp_path, sheet_name=\"Hours\", header=[8, 9, 10, 11])\n",
    "\n",
    "\n",
    "def tidy(col):\n",
    "    parts = [\n",
    "        str(x).strip()\n",
    "        for x in col\n",
    "        if str(x).strip() not in {\"nan\", \"\"} and not str(x).startswith(\"Unnamed\")\n",
    "    ]\n",
    "    return \"_\".join(parts).strip(\"_\")\n",
    "\n",
    "\n",
    "ctp_hours.columns = [tidy(col) for col in ctp_hours.columns]\n",
    "\n",
    "ctp_hours = ctp_hours.rename(columns={\"Project\": \"Project ID\"})\n",
    "\n",
    "mask = ctp_hours[\"Project ID\"].astype(str).str.strip().str.lower().str.endswith(\"total\")\n",
    "ctp_hours = ctp_hours[mask].copy()\n",
    "ctp_hours.reset_index(drop=True, inplace=True)\n",
    "\n",
    "rng_mask = (\n",
    "    ctp_hours[\"Project ID\"].astype(str).str.match(r\"^(RNG\\d+)\\s+Total$\", na=False)\n",
    ")\n",
    "ctp_hours.loc[rng_mask, \"Project ID\"] = (\n",
    "    ctp_hours.loc[rng_mask, \"Project ID\"]\n",
    "    .astype(str)\n",
    "    .str.replace(r\"^(RNG\\d+)\\s+Total$\", r\"\\1\", regex=True)\n",
    "    .str.strip()\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Columns:\", ctp_hours.columns.tolist())\n",
    "\n",
    "print(\"Loaded dataframe shape:\", ctp_hours.shape)\n",
    "\n",
    "ctp_hours.to_excel(\n",
    "    \"C:\\\\Users\\\\O304312\\\\OneDrive - Kaiser Permanente\\\\Documents\\\\Tableau Dashboards\\\\New Financial Snapshot\\\\Data\\\\DOR Personnel.xlsx\",\n",
    "    index=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bb41d445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: J:\\VIEWPOINT\\SiteStudyDetails_Response\\2025-12-05-SiteStudyDetails.json\n",
      "Converted to DataFrame with shape: (1078, 40)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "vp_folder = Path(\"J:\\\\VIEWPOINT\\\\SiteStudyDetails_Response\")\n",
    "\n",
    "pattern = re.compile(r\"^\\d{4}-\\d{2}-\\d{2}-SiteStudyDetails\\.json$\")\n",
    "\n",
    "matches = [p for p in vp_folder.iterdir() if p.is_file() and pattern.match(p.name)]\n",
    "if not matches:\n",
    "    raise FileNotFoundError(f\"No SiteStudyDetails json files found in {vp_folder!s}\")\n",
    "\n",
    "latest_json = max(matches, key=lambda p: p.stat().st_mtime)\n",
    "print(\"Loading:\", latest_json)\n",
    "\n",
    "with latest_json.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    site_details = json.load(f)\n",
    "\n",
    "try:\n",
    "    ss_df = pd.json_normalize(site_details)\n",
    "    print(\"Converted to DataFrame with shape:\", ss_df.shape)\n",
    "except Exception:\n",
    "    ss_df = None\n",
    "    print(\"JSON loaded into 'site_details' (not converted to DataFrame).\")\n",
    "\n",
    "vp_study_details = ss_df.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f49f7e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: J:\\VIEWPOINT\\Accountables_Response\\2025-12-05-Accountables.json\n",
      "Converted to DataFrame with shape: (12089, 38)\n"
     ]
    }
   ],
   "source": [
    "vp_accountables_folder = Path(\"J:\\\\VIEWPOINT\\\\Accountables_Response\")\n",
    "\n",
    "pattern = re.compile(r\"^\\d{4}-\\d{2}-\\d{2}-Accountables\\.json$\")\n",
    "\n",
    "matches = [\n",
    "    p for p in vp_accountables_folder.iterdir() if p.is_file() and pattern.match(p.name)\n",
    "]\n",
    "if not matches:\n",
    "    raise FileNotFoundError(\n",
    "        f\"No SiteStudyDetails json files found in {vp_accountables_folder!s}\"\n",
    "    )\n",
    "\n",
    "latest_json = max(matches, key=lambda p: p.stat().st_mtime)\n",
    "print(\"Loading:\", latest_json)\n",
    "\n",
    "with latest_json.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    site_details = json.load(f)\n",
    "\n",
    "try:\n",
    "    account = pd.json_normalize(site_details)\n",
    "    print(\"Converted to DataFrame with shape:\", account.shape)\n",
    "except Exception:\n",
    "    account = None\n",
    "    print(\"JSON loaded into 'site_details' (not converted to DataFrame).\")\n",
    "\n",
    "vp_accountables = account.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b0ab9e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "vp_accountables = vp_accountables.merge(\n",
    "    vp_study_details[[\"site_study_service_line\", \"network_study_uuid\"]],\n",
    "    on=\"network_study_uuid\",\n",
    "    how=\"left\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3c8e007a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse completion_date safely and compare against a Timestamp cutoff\n",
    "vp_accountables[\"completion_date_parsed\"] = pd.to_datetime(\n",
    "    vp_accountables[\"completion_date\"], errors=\"coerce\"\n",
    ")\n",
    "\n",
    "# ensure dor_end_date is a pandas Timestamp (not a python date)\n",
    "cutoff = pd.to_datetime(dor_end_date)\n",
    "\n",
    "vp_accountables = vp_accountables[\n",
    "    vp_accountables[\"completion_date_parsed\"] < cutoff\n",
    "].copy()\n",
    "\n",
    "# remove helper column\n",
    "vp_accountables.drop(columns=[\"completion_date_parsed\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2fdfa4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "vp_accountables = vp_accountables[[\"site_study_service_line\", \"amount\"]]\n",
    "vp_accountables = vp_accountables.groupby(\"site_study_service_line\").sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "70dc039c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove control characters that cause openpyxl IllegalCharacterError\n",
    "# keep common whitespace (\\t, \\n, \\r); remove other < 0x20 controls\n",
    "ctl_re = re.compile(r\"[\\x00-\\x08\\x0b\\x0c\\x0e-\\x1f]\")\n",
    "for col in vp_accountables.select_dtypes(include=[\"object\"]).columns:\n",
    "    mask = vp_accountables[col].notna()\n",
    "    vp_accountables.loc[mask, col] = (\n",
    "        vp_accountables.loc[mask, col].astype(str).map(lambda s: ctl_re.sub(\"\", s))\n",
    "    )\n",
    "\n",
    "vp_accountables.to_excel(\n",
    "    r\"C:\\Users\\O304312\\OneDrive - Kaiser Permanente\\Documents\\Tableau Dashboards\\New Financial Snapshot\\Data\\Viewpoint Accountables.xlsx\",\n",
    "    index=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cc93d6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "vp_study_details.to_excel(\n",
    "    r\"C:\\Users\\O304312\\OneDrive - Kaiser Permanente\\Documents\\Tableau Dashboards\\New Financial Snapshot\\Data\\Viewpoint Site Study Details.xlsx\",\n",
    "    index=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "852dc1f8",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Project Title'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\O304312\\Documents\\Github\\kp_data_pipelines\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3811\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'Project Title'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m dor[\u001b[33m\"\u001b[39m\u001b[33mID\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43mdor\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mProject Title\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m      3\u001b[39m program_col = \u001b[38;5;28mnext\u001b[39m(\n\u001b[32m      4\u001b[39m     (c \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m dor.columns \u001b[38;5;28;01mif\u001b[39;00m normalize(c) == normalize(\u001b[33m\"\u001b[39m\u001b[33mProgram Area\u001b[39m\u001b[33m\"\u001b[39m)), \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m      5\u001b[39m )\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m program_col \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\O304312\\Documents\\Github\\kp_data_pipelines\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py:4107\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4105\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4106\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4107\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4108\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4109\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\O304312\\Documents\\Github\\kp_data_pipelines\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3819\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3814\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3815\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3816\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3817\u001b[39m     ):\n\u001b[32m   3818\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3819\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3820\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3821\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3822\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3823\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3824\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'Project Title'"
     ]
    }
   ],
   "source": [
    "dor[\"ID\"] = dor[\"Project Title\"]\n",
    "\n",
    "program_col = next(\n",
    "    (c for c in dor.columns if normalize(c) == normalize(\"Program Area\")), None\n",
    ")\n",
    "\n",
    "if program_col is not None:\n",
    "\n",
    "    def _remove_prog(id_val, prog_val):\n",
    "        if pd.isna(id_val) or pd.isna(prog_val):\n",
    "            return id_val\n",
    "        id_s = str(id_val)\n",
    "        prog_s = str(prog_val).strip()\n",
    "        if not prog_s:\n",
    "            return id_s\n",
    "        out = re.sub(re.escape(prog_s), \"\", id_s, flags=re.IGNORECASE)\n",
    "        out = re.sub(r\"[\\-\\–\\—:;\\/]+\", \" \", out)\n",
    "        out = \" \".join(out.split()).strip()\n",
    "        return out\n",
    "\n",
    "    dor[\"ID\"] = dor.apply(lambda r: _remove_prog(r[\"ID\"], r[program_col]), axis=1)\n",
    "else:\n",
    "    print(\"Warning: 'Program Area' column not found in dor; ID left unchanged.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7c219e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153 Project ID(s) not found in site_study_service_line (saved to file)\n"
     ]
    }
   ],
   "source": [
    "vp_set = set(\n",
    "    vp_study_details[\"site_study_service_line\"]\n",
    "    .dropna()\n",
    "    .astype(str)\n",
    "    .map(lambda s: \"\".join(s.split()).casefold())\n",
    ")\n",
    "\n",
    "dor_norm = dor[\"Project ID\"].astype(str).map(lambda s: \"\".join(s.split()).casefold())\n",
    "\n",
    "mask_missing = ~dor_norm.isin(vp_set)\n",
    "\n",
    "title_col = next(\n",
    "    (c for c in dor.columns if normalize(c) == normalize(\"Project Title\")), None\n",
    ")\n",
    "\n",
    "cols = [\"Project ID\"]\n",
    "if title_col:\n",
    "    cols.append(title_col)\n",
    "\n",
    "missing_df = dor.loc[mask_missing, cols].drop_duplicates().copy()\n",
    "\n",
    "vp_map = {}\n",
    "for sl, sid in (\n",
    "    vp_study_details[[\"site_study_service_line\", \"site_study_id\"]]\n",
    "    .dropna(subset=[\"site_study_service_line\"])\n",
    "    .itertuples(index=False)\n",
    "):\n",
    "    key = \"\".join(str(sl).split()).casefold()\n",
    "    if key and pd.notna(sid):\n",
    "        # keep first occurrence\n",
    "        vp_map.setdefault(key, sid)\n",
    "\n",
    "missing_df[\"site_study_id\"] = (\n",
    "    missing_df[\"Project ID\"]\n",
    "    .astype(str)\n",
    "    .map(lambda s: \"\".join(s.split()).casefold())\n",
    "    .map(vp_map)\n",
    ")\n",
    "\n",
    "if title_col:\n",
    "    missing_df[\"site_study_id\"] = missing_df[\"site_study_id\"].fillna(\n",
    "        missing_df[title_col]\n",
    "    )\n",
    "\n",
    "out_cols = [\"site_study_id\", \"Project ID\"]\n",
    "if title_col:\n",
    "    out_cols.append(title_col)\n",
    "\n",
    "missing_out = missing_df.loc[:, out_cols]\n",
    "\n",
    "print(\n",
    "    f\"{len(missing_out)} Project ID(s) not found in site_study_service_line (saved to file)\"\n",
    ")\n",
    "\n",
    "out_path = r\"C:\\Users\\O304312\\OneDrive - Kaiser Permanente\\Documents\\Tableau Dashboards\\New Financial Snapshot\\Data\\VP Missing RNG Numbers.xlsx\"\n",
    "missing_out.to_excel(out_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcfe1a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
