{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54defb73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting trl>=0.20.0\n",
      "  Downloading trl-0.25.1-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting peft>=0.17.0\n",
      "  Downloading peft-0.18.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting transformers>=4.55.0\n",
      "  Downloading transformers-4.57.3-py3-none-any.whl.metadata (43 kB)\n",
      "Collecting accelerate>=1.4.0 (from trl>=0.20.0)\n",
      "  Downloading accelerate-1.12.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting datasets>=3.0.0 (from trl>=0.20.0)\n",
      "  Downloading datasets-4.4.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\o304312\\documents\\github\\kp_data_pipelines\\.venv\\lib\\site-packages (from peft>=0.17.0) (2.3.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\o304312\\documents\\github\\kp_data_pipelines\\.venv\\lib\\site-packages (from peft>=0.17.0) (25.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\o304312\\documents\\github\\kp_data_pipelines\\.venv\\lib\\site-packages (from peft>=0.17.0) (7.0.0)\n",
      "Collecting pyyaml (from peft>=0.17.0)\n",
      "  Using cached pyyaml-6.0.3-cp313-cp313-win_amd64.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: torch>=1.13.0 in c:\\users\\o304312\\documents\\github\\kp_data_pipelines\\.venv\\lib\\site-packages (from peft>=0.17.0) (2.9.1+cu126)\n",
      "Collecting tqdm (from peft>=0.17.0)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting safetensors (from peft>=0.17.0)\n",
      "  Downloading safetensors-0.7.0-cp38-abi3-win_amd64.whl.metadata (4.2 kB)\n",
      "Collecting huggingface_hub>=0.25.0 (from peft>=0.17.0)\n",
      "  Downloading huggingface_hub-1.2.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\o304312\\documents\\github\\kp_data_pipelines\\.venv\\lib\\site-packages (from transformers>=4.55.0) (3.19.1)\n",
      "  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers>=4.55.0)\n",
      "  Downloading regex-2025.11.3-cp313-cp313-win_amd64.whl.metadata (41 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\o304312\\documents\\github\\kp_data_pipelines\\.venv\\lib\\site-packages (from transformers>=4.55.0) (2.32.5)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers>=4.55.0)\n",
      "  Downloading tokenizers-0.22.1-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\o304312\\documents\\github\\kp_data_pipelines\\.venv\\lib\\site-packages (from huggingface_hub>=0.25.0->peft>=0.17.0) (2025.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\o304312\\documents\\github\\kp_data_pipelines\\.venv\\lib\\site-packages (from huggingface_hub>=0.25.0->peft>=0.17.0) (4.15.0)\n",
      "Collecting pyarrow>=21.0.0 (from datasets>=3.0.0->trl>=0.20.0)\n",
      "  Downloading pyarrow-22.0.0-cp313-cp313-win_amd64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.4.1,>=0.3.0 (from datasets>=3.0.0->trl>=0.20.0)\n",
      "  Using cached dill-0.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\o304312\\documents\\github\\kp_data_pipelines\\.venv\\lib\\site-packages (from datasets>=3.0.0->trl>=0.20.0) (2.3.2)\n",
      "Collecting httpx<1.0.0 (from datasets>=3.0.0->trl>=0.20.0)\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting xxhash (from datasets>=3.0.0->trl>=0.20.0)\n",
      "  Downloading xxhash-3.6.0-cp313-cp313-win_amd64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.19 (from datasets>=3.0.0->trl>=0.20.0)\n",
      "  Downloading multiprocess-0.70.18-py313-none-any.whl.metadata (7.2 kB)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0->trl>=0.20.0)\n",
      "  Downloading aiohttp-3.13.2-cp313-cp313-win_amd64.whl.metadata (8.4 kB)\n",
      "Collecting anyio (from httpx<1.0.0->datasets>=3.0.0->trl>=0.20.0)\n",
      "  Downloading anyio-4.12.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: certifi in c:\\users\\o304312\\documents\\github\\kp_data_pipelines\\.venv\\lib\\site-packages (from httpx<1.0.0->datasets>=3.0.0->trl>=0.20.0) (2025.11.12)\n",
      "Collecting httpcore==1.* (from httpx<1.0.0->datasets>=3.0.0->trl>=0.20.0)\n",
      "  Using cached httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: idna in c:\\users\\o304312\\documents\\github\\kp_data_pipelines\\.venv\\lib\\site-packages (from httpx<1.0.0->datasets>=3.0.0->trl>=0.20.0) (3.11)\n",
      "Collecting h11>=0.16 (from httpcore==1.*->httpx<1.0.0->datasets>=3.0.0->trl>=0.20.0)\n",
      "  Using cached h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0->trl>=0.20.0)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0->trl>=0.20.0)\n",
      "  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0->trl>=0.20.0)\n",
      "  Using cached attrs-25.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0->trl>=0.20.0)\n",
      "  Downloading frozenlist-1.8.0-cp313-cp313-win_amd64.whl.metadata (21 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0->trl>=0.20.0)\n",
      "  Downloading multidict-6.7.0-cp313-cp313-win_amd64.whl.metadata (5.5 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0->trl>=0.20.0)\n",
      "  Downloading propcache-0.4.1-cp313-cp313-win_amd64.whl.metadata (14 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0->trl>=0.20.0)\n",
      "  Downloading yarl-1.22.0-cp313-cp313-win_amd64.whl.metadata (77 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\o304312\\documents\\github\\kp_data_pipelines\\.venv\\lib\\site-packages (from requests->transformers>=4.55.0) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\o304312\\documents\\github\\kp_data_pipelines\\.venv\\lib\\site-packages (from requests->transformers>=4.55.0) (2.5.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\o304312\\documents\\github\\kp_data_pipelines\\.venv\\lib\\site-packages (from torch>=1.13.0->peft>=0.17.0) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\o304312\\documents\\github\\kp_data_pipelines\\.venv\\lib\\site-packages (from torch>=1.13.0->peft>=0.17.0) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\o304312\\documents\\github\\kp_data_pipelines\\.venv\\lib\\site-packages (from torch>=1.13.0->peft>=0.17.0) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\o304312\\documents\\github\\kp_data_pipelines\\.venv\\lib\\site-packages (from torch>=1.13.0->peft>=0.17.0) (70.2.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\o304312\\documents\\github\\kp_data_pipelines\\.venv\\lib\\site-packages (from sympy>=1.13.3->torch>=1.13.0->peft>=0.17.0) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\o304312\\documents\\github\\kp_data_pipelines\\.venv\\lib\\site-packages (from tqdm->peft>=0.17.0) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\o304312\\documents\\github\\kp_data_pipelines\\.venv\\lib\\site-packages (from jinja2->torch>=1.13.0->peft>=0.17.0) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\o304312\\documents\\github\\kp_data_pipelines\\.venv\\lib\\site-packages (from pandas->datasets>=3.0.0->trl>=0.20.0) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\o304312\\documents\\github\\kp_data_pipelines\\.venv\\lib\\site-packages (from pandas->datasets>=3.0.0->trl>=0.20.0) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\o304312\\documents\\github\\kp_data_pipelines\\.venv\\lib\\site-packages (from pandas->datasets>=3.0.0->trl>=0.20.0) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\o304312\\documents\\github\\kp_data_pipelines\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets>=3.0.0->trl>=0.20.0) (1.17.0)\n",
      "Downloading trl-0.25.1-py3-none-any.whl (465 kB)\n",
      "Downloading peft-0.18.0-py3-none-any.whl (556 kB)\n",
      "   ---------------------------------------- 0.0/556.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 556.4/556.4 kB 9.2 MB/s  0:00:00\n",
      "Downloading transformers-4.57.3-py3-none-any.whl (12.0 MB)\n",
      "   ---------------------------------------- 0.0/12.0 MB ? eta -:--:--\n",
      "   ------------------------ --------------- 7.3/12.0 MB 37.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.0/12.0 MB 44.5 MB/s  0:00:00\n",
      "Downloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
      "   ---------------------------------------- 0.0/566.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 566.1/566.1 kB 26.4 MB/s  0:00:00\n",
      "Downloading tokenizers-0.22.1-cp39-abi3-win_amd64.whl (2.7 MB)\n",
      "   ---------------------------------------- 0.0/2.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.7/2.7 MB 78.6 MB/s  0:00:00\n",
      "Downloading accelerate-1.12.0-py3-none-any.whl (380 kB)\n",
      "Downloading datasets-4.4.1-py3-none-any.whl (511 kB)\n",
      "Using cached dill-0.4.0-py3-none-any.whl (119 kB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Using cached httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Downloading multiprocess-0.70.18-py313-none-any.whl (151 kB)\n",
      "Downloading aiohttp-3.13.2-cp313-cp313-win_amd64.whl (452 kB)\n",
      "Downloading multidict-6.7.0-cp313-cp313-win_amd64.whl (45 kB)\n",
      "Downloading yarl-1.22.0-cp313-cp313-win_amd64.whl (86 kB)\n",
      "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Using cached attrs-25.4.0-py3-none-any.whl (67 kB)\n",
      "Downloading frozenlist-1.8.0-cp313-cp313-win_amd64.whl (43 kB)\n",
      "Using cached h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Downloading propcache-0.4.1-cp313-cp313-win_amd64.whl (40 kB)\n",
      "Downloading pyarrow-22.0.0-cp313-cp313-win_amd64.whl (28.0 MB)\n",
      "   ---------------------------------------- 0.0/28.0 MB ? eta -:--:--\n",
      "   -------------------------------- ------- 22.8/28.0 MB 111.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 28.0/28.0 MB 90.1 MB/s  0:00:00\n",
      "Using cached pyyaml-6.0.3-cp313-cp313-win_amd64.whl (154 kB)\n",
      "Downloading regex-2025.11.3-cp313-cp313-win_amd64.whl (277 kB)\n",
      "Downloading safetensors-0.7.0-cp38-abi3-win_amd64.whl (341 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading anyio-4.12.0-py3-none-any.whl (113 kB)\n",
      "Downloading xxhash-3.6.0-cp313-cp313-win_amd64.whl (31 kB)\n",
      "Installing collected packages: xxhash, tqdm, safetensors, regex, pyyaml, pyarrow, propcache, multidict, h11, frozenlist, dill, attrs, anyio, aiohappyeyeballs, yarl, multiprocess, huggingface_hub, httpcore, aiosignal, tokenizers, httpx, aiohttp, accelerate, transformers, peft, datasets, trl\n",
      "\n",
      "   - --------------------------------------  1/27 [tqdm]\n",
      "   ---- -----------------------------------  3/27 [regex]\n",
      "   ------- --------------------------------  5/27 [pyarrow]\n",
      "   ------- --------------------------------  5/27 [pyarrow]\n",
      "   ------- --------------------------------  5/27 [pyarrow]\n",
      "   ------- --------------------------------  5/27 [pyarrow]\n",
      "   ------- --------------------------------  5/27 [pyarrow]\n",
      "   ------- --------------------------------  5/27 [pyarrow]\n",
      "   ------- --------------------------------  5/27 [pyarrow]\n",
      "   ------- --------------------------------  5/27 [pyarrow]\n",
      "   ------- --------------------------------  5/27 [pyarrow]\n",
      "   ------- --------------------------------  5/27 [pyarrow]\n",
      "   ------- --------------------------------  5/27 [pyarrow]\n",
      "   ------- --------------------------------  5/27 [pyarrow]\n",
      "   ------- --------------------------------  5/27 [pyarrow]\n",
      "   ---------- -----------------------------  7/27 [multidict]\n",
      "   -------------- ------------------------- 10/27 [dill]\n",
      "   -------------- ------------------------- 10/27 [dill]\n",
      "   ---------------- ----------------------- 11/27 [attrs]\n",
      "   ----------------- ---------------------- 12/27 [anyio]\n",
      "   -------------------- ------------------- 14/27 [yarl]\n",
      "   ---------------------- ----------------- 15/27 [multiprocess]\n",
      "   ----------------------- ---------------- 16/27 [huggingface_hub]\n",
      "   ----------------------- ---------------- 16/27 [huggingface_hub]\n",
      "   ----------------------- ---------------- 16/27 [huggingface_hub]\n",
      "   ----------------------- ---------------- 16/27 [huggingface_hub]\n",
      "   ----------------------- ---------------- 16/27 [huggingface_hub]\n",
      "   ------------------------- -------------- 17/27 [httpcore]\n",
      "   -------------------------- ------------- 18/27 [aiosignal]\n",
      "   ---------------------------- ----------- 19/27 [tokenizers]\n",
      "   ----------------------------- ---------- 20/27 [httpx]\n",
      "   ------------------------------- -------- 21/27 [aiohttp]\n",
      "   ------------------------------- -------- 21/27 [aiohttp]\n",
      "   -------------------------------- ------- 22/27 [accelerate]\n",
      "   -------------------------------- ------- 22/27 [accelerate]\n",
      "   -------------------------------- ------- 22/27 [accelerate]\n",
      "   ---------------------------------- ----- 23/27 [transformers]\n",
      "   ---------------------------------- ----- 23/27 [transformers]\n",
      "   ---------------------------------- ----- 23/27 [transformers]\n",
      "   ---------------------------------- ----- 23/27 [transformers]\n",
      "   ---------------------------------- ----- 23/27 [transformers]\n",
      "   ---------------------------------- ----- 23/27 [transformers]\n",
      "   ---------------------------------- ----- 23/27 [transformers]\n",
      "   ---------------------------------- ----- 23/27 [transformers]\n",
      "   ---------------------------------- ----- 23/27 [transformers]\n",
      "   ---------------------------------- ----- 23/27 [transformers]\n",
      "   ---------------------------------- ----- 23/27 [transformers]\n",
      "   ---------------------------------- ----- 23/27 [transformers]\n",
      "   ---------------------------------- ----- 23/27 [transformers]\n",
      "   ---------------------------------- ----- 23/27 [transformers]\n",
      "   ---------------------------------- ----- 23/27 [transformers]\n",
      "   ---------------------------------- ----- 23/27 [transformers]\n",
      "   ---------------------------------- ----- 23/27 [transformers]\n",
      "   ---------------------------------- ----- 23/27 [transformers]\n",
      "   ---------------------------------- ----- 23/27 [transformers]\n",
      "   ---------------------------------- ----- 23/27 [transformers]\n",
      "   ---------------------------------- ----- 23/27 [transformers]\n",
      "   ---------------------------------- ----- 23/27 [transformers]\n",
      "   ---------------------------------- ----- 23/27 [transformers]\n",
      "   ---------------------------------- ----- 23/27 [transformers]\n",
      "   ---------------------------------- ----- 23/27 [transformers]\n",
      "   ---------------------------------- ----- 23/27 [transformers]\n",
      "   ---------------------------------- ----- 23/27 [transformers]\n",
      "   ---------------------------------- ----- 23/27 [transformers]\n",
      "   ---------------------------------- ----- 23/27 [transformers]\n",
      "   ---------------------------------- ----- 23/27 [transformers]\n",
      "   ---------------------------------- ----- 23/27 [transformers]\n",
      "   ---------------------------------- ----- 23/27 [transformers]\n",
      "   ---------------------------------- ----- 23/27 [transformers]\n",
      "   ---------------------------------- ----- 23/27 [transformers]\n",
      "   ---------------------------------- ----- 23/27 [transformers]\n",
      "   ---------------------------------- ----- 23/27 [transformers]\n",
      "   ---------------------------------- ----- 23/27 [transformers]\n",
      "   ---------------------------------- ----- 23/27 [transformers]\n",
      "   ---------------------------------- ----- 23/27 [transformers]\n",
      "   ---------------------------------- ----- 23/27 [transformers]\n",
      "   ---------------------------------- ----- 23/27 [transformers]\n",
      "   ---------------------------------- ----- 23/27 [transformers]\n",
      "   ---------------------------------- ----- 23/27 [transformers]\n",
      "   ---------------------------------- ----- 23/27 [transformers]\n",
      "   ---------------------------------- ----- 23/27 [transformers]\n",
      "   ---------------------------------- ----- 23/27 [transformers]\n",
      "   ---------------------------------- ----- 23/27 [transformers]\n",
      "   ---------------------------------- ----- 23/27 [transformers]\n",
      "   ---------------------------------- ----- 23/27 [transformers]\n",
      "   ---------------------------------- ----- 23/27 [transformers]\n",
      "   ---------------------------------- ----- 23/27 [transformers]\n",
      "   ---------------------------------- ----- 23/27 [transformers]\n",
      "   ---------------------------------- ----- 23/27 [transformers]\n",
      "   ---------------------------------- ----- 23/27 [transformers]\n",
      "   ---------------------------------- ----- 23/27 [transformers]\n",
      "   ---------------------------------- ----- 23/27 [transformers]\n",
      "   ---------------------------------- ----- 23/27 [transformers]\n",
      "   ---------------------------------- ----- 23/27 [transformers]\n",
      "   ---------------------------------- ----- 23/27 [transformers]\n",
      "   ---------------------------------- ----- 23/27 [transformers]\n",
      "   ---------------------------------- ----- 23/27 [transformers]\n",
      "   ---------------------------------- ----- 23/27 [transformers]\n",
      "   ---------------------------------- ----- 23/27 [transformers]\n",
      "   ---------------------------------- ----- 23/27 [transformers]\n",
      "   ---------------------------------- ----- 23/27 [transformers]\n",
      "   ---------------------------------- ----- 23/27 [transformers]\n",
      "   ---------------------------------- ----- 23/27 [transformers]\n",
      "   ---------------------------------- ----- 23/27 [transformers]\n",
      "   ---------------------------------- ----- 23/27 [transformers]\n",
      "   ---------------------------------- ----- 23/27 [transformers]\n",
      "   ---------------------------------- ----- 23/27 [transformers]\n",
      "   ---------------------------------- ----- 23/27 [transformers]\n",
      "   ---------------------------------- ----- 23/27 [transformers]\n",
      "   ---------------------------------- ----- 23/27 [transformers]\n",
      "   ---------------------------------- ----- 23/27 [transformers]\n",
      "   ---------------------------------- ----- 23/27 [transformers]\n",
      "   ---------------------------------- ----- 23/27 [transformers]\n",
      "   ---------------------------------- ----- 23/27 [transformers]\n",
      "   ---------------------------------- ----- 23/27 [transformers]\n",
      "   ---------------------------------- ----- 23/27 [transformers]\n",
      "   ---------------------------------- ----- 23/27 [transformers]\n",
      "   ---------------------------------- ----- 23/27 [transformers]\n",
      "   ---------------------------------- ----- 23/27 [transformers]\n",
      "   ---------------------------------- ----- 23/27 [transformers]\n",
      "   ---------------------------------- ----- 23/27 [transformers]\n",
      "   ---------------------------------- ----- 23/27 [transformers]\n",
      "   ---------------------------------- ----- 23/27 [transformers]\n",
      "   ---------------------------------- ----- 23/27 [transformers]\n",
      "   ---------------------------------- ----- 23/27 [transformers]\n",
      "   ----------------------------------- ---- 24/27 [peft]\n",
      "   ----------------------------------- ---- 24/27 [peft]\n",
      "   ----------------------------------- ---- 24/27 [peft]\n",
      "   ----------------------------------- ---- 24/27 [peft]\n",
      "   ----------------------------------- ---- 24/27 [peft]\n",
      "   ----------------------------------- ---- 24/27 [peft]\n",
      "   ------------------------------------- -- 25/27 [datasets]\n",
      "   ------------------------------------- -- 25/27 [datasets]\n",
      "   ------------------------------------- -- 25/27 [datasets]\n",
      "   ------------------------------------- -- 25/27 [datasets]\n",
      "   ------------------------------------- -- 25/27 [datasets]\n",
      "   -------------------------------------- - 26/27 [trl]\n",
      "   -------------------------------------- - 26/27 [trl]\n",
      "   -------------------------------------- - 26/27 [trl]\n",
      "   ---------------------------------------- 27/27 [trl]\n",
      "\n",
      "Successfully installed accelerate-1.12.0 aiohappyeyeballs-2.6.1 aiohttp-3.13.2 aiosignal-1.4.0 anyio-4.12.0 attrs-25.4.0 datasets-4.4.1 dill-0.4.0 frozenlist-1.8.0 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1 huggingface_hub-0.36.0 multidict-6.7.0 multiprocess-0.70.18 peft-0.18.0 propcache-0.4.1 pyarrow-22.0.0 pyyaml-6.0.3 regex-2025.11.3 safetensors-0.7.0 tokenizers-0.22.1 tqdm-4.67.1 transformers-4.57.3 trl-0.25.1 xxhash-3.6.0 yarl-1.22.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\O304312\\Documents\\Github\\kp_data_pipelines\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\O304312\\Documents\\Github\\kp_data_pipelines\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\O304312\\.cache\\huggingface\\hub\\models--openai--gpt-oss-20b. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Using MXFP4 quantized models requires a GPU, we will default to dequantizing the model to bf16\n",
      "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Fetching 3 files: 100%|██████████| 3/3 [02:06<00:00, 42.21s/it] \n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:15<00:00,  5.02s/it]\n",
      "Some parameters are on the meta device because they were offloaded to the cpu and disk.\n",
      "c:\\Users\\O304312\\Documents\\Github\\kp_data_pipelines\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\O304312\\.cache\\huggingface\\hub\\models--dousery--medical-reasoning-gpt-oss-20b. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'model.layers.4.mlp.experts.gate_up_proj'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     12\u001b[39m tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n\u001b[32m     14\u001b[39m base_model = AutoModelForCausalLM.from_pretrained(\n\u001b[32m     15\u001b[39m     base_model_name,\n\u001b[32m     16\u001b[39m     torch_dtype=torch.bfloat16,\n\u001b[32m     17\u001b[39m     device_map=\u001b[33m\"\u001b[39m\u001b[33mauto\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     18\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m model = \u001b[43mPeftModel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m model = model.merge_and_unload()\n\u001b[32m     23\u001b[39m messages = [\n\u001b[32m     24\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33msystem\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mYou are a medical reasoning assistant.\u001b[39m\u001b[33m\"\u001b[39m},\n\u001b[32m     25\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: (\n\u001b[32m   (...)\u001b[39m\u001b[32m     28\u001b[39m     )}\n\u001b[32m     29\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\O304312\\Documents\\Github\\kp_data_pipelines\\.venv\\Lib\\site-packages\\peft\\peft_model.py:508\u001b[39m, in \u001b[36mPeftModel.from_pretrained\u001b[39m\u001b[34m(cls, model, model_id, adapter_name, is_trainable, config, autocast_adapter_dtype, ephemeral_gpu_offload, low_cpu_mem_usage, key_mapping, **kwargs)\u001b[39m\n\u001b[32m    503\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mDisk offloading currently only supported for safetensors\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    505\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m index:\n\u001b[32m    506\u001b[39m         offload_index = {\n\u001b[32m    507\u001b[39m             p: {\n\u001b[32m--> \u001b[39m\u001b[32m508\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33msafetensors_file\u001b[39m\u001b[33m\"\u001b[39m: \u001b[43mindex\u001b[49m\u001b[43m[\u001b[49m\u001b[43mp\u001b[49m\u001b[43m]\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33msafetensors_file\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    509\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mweight_name\u001b[39m\u001b[33m\"\u001b[39m: p,\n\u001b[32m    510\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(weight_map[p].dtype).replace(\u001b[33m\"\u001b[39m\u001b[33mtorch.\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    511\u001b[39m             }\n\u001b[32m    512\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m weight_map.keys()\n\u001b[32m    513\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m disk_modules\n\u001b[32m    514\u001b[39m         }\n\u001b[32m    515\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33moffload_index\u001b[39m\u001b[33m\"\u001b[39m] = offload_index\n\u001b[32m    517\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mgetattr\u001b[39m(model, \u001b[33m\"\u001b[39m\u001b[33mhf_device_map\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\n\u001b[32m    518\u001b[39m     \u001b[38;5;28mset\u001b[39m(model.hf_device_map.values()).intersection({\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mdisk\u001b[39m\u001b[33m\"\u001b[39m})\n\u001b[32m    519\u001b[39m ) > \u001b[32m0\u001b[39m:\n",
      "\u001b[31mKeyError\u001b[39m: 'model.layers.4.mlp.experts.gate_up_proj'"
     ]
    }
   ],
   "source": [
    "#pip install torch --index-url https://download.pytorch.org/whl/cu128\n",
    "%pip install \"trl>=0.20.0\" \"peft>=0.17.0\" \"transformers>=4.55.0\"\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "import re\n",
    "\n",
    "base_model_name = \"openai/gpt-oss-20b\"\n",
    "adapter_name = \"dousery/medical-reasoning-gpt-oss-20b\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "model = PeftModel.from_pretrained(base_model, adapter_name)\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a medical reasoning assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": (\n",
    "        \"\"\"A 55-year-old man has chest pain and elevated troponin I without ST elevation.\n",
    "         What is the diagnosis and what additional test would you order next?\"\"\"\n",
    "    )}\n",
    "]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=2048,\n",
    "    temperature=0.2,\n",
    "    do_sample=False\n",
    ")\n",
    "\n",
    "raw_output = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "\n",
    "#  PARSING THE OUTPUT\n",
    "thinking_pattern = r\"<\\|end\\|><\\|start\\|>assistant<\\|channel\\|>analysis<\\|message\\|>(.*?)<\\|end\\|>\"\n",
    "final_pattern = r\"<\\|start\\|>assistant<\\|channel\\|>final<\\|message\\|>(.*?)<\\|return\\|>\"\n",
    "\n",
    "thinking_match = re.search(thinking_pattern, raw_output, re.DOTALL)\n",
    "final_match = re.search(final_pattern, raw_output, re.DOTALL)\n",
    "\n",
    "thinking_text = thinking_match.group(1).strip() if thinking_match else \"N/A\"\n",
    "final_text = final_match.group(1).strip() if final_match else \"N/A\"\n",
    "\n",
    "print(\"Thinking:\", thinking_text)\n",
    "print(\"\\nFinal:\", final_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
